# Что

Небольшая ученическая программа, реализующая модель машинного обучения Random Forest по типу CART. Для реального, практического использования, конечено, лучше испаользовать не pure-python реализацию, а, например, [эту](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn-ensemble-randomforestclassifier) реализацию в [sklearn](https://scikit-learn.org/stable/about.html)

Поскольку проект давний, ученический, фактически являющийся лабораторной по выбору, то о никаком MVC и прочей архитектуре речи не идёт. В cuteform.py спагеттирована вся логика вьюхи, а в descisiontree.py логика работы с деревьями. Самый значимый момент, а заодно и самый короткий это формула чистоты джини: [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), которая лижит в виде def функции в самом верху файла descisiontree.py

# Запуск

В корне лежит файл rf2016.yml - это окружение anaconda.
Качаем [анаконду](https://docs.anaconda.com/anaconda/install/) полная ли версия, или миниконда — без разницы.

В папке, где лежит rf2016.yml:

> conda env create -f rf2016.yml

На префикс в файле внимания не обращаем: конда делает его сама, а потом ей же самой на него плевать.
После этого

> conda activate random_forest_2016

Ну и запускам:

> python ./RandomForest/main.py

# Использование

<div style="text-align:right;"><blockquote>Не надо</blockquote> <i>~Парень, который не хотел умирац</i></div>  
  
</br>

## Используем готовый проект

По вкладке "Помощь" ничего не открывается, потому-что мы не в сказке живём. Во вкладке "Файл" есть разделы открытия/сохранения и загрузки проекта. Все проеты, которые сейчас есть хранятся в папке proj. Проект сохраняется как JSON, например RandomForest/proj/pokemon_analyzer/pokemon_analizer.json

Если его открыть, то в правом окне приложения будут расположены виджеты деревьев. Строка с названием сохранённого дерева, чекбокс, который при активации сообщает о том, что дерево будет использоваться в ансамбле, синяя иконка глаза, нажав на которую можно поглазеть на дерево, отрисованное в матплотлиб. Да, никакого [Graphviz](https://www.graphviz.org/), а жаль. Внизу есть кнопки тестирования выбранного ансабля на заданном датасете и сравнение эффективности деревьев по-отдельности в виде столбчатой диаграммы. ЕМНИП на голосование эффективность отдельного дерева не влияет.

В левой панели отображается название базы данных, имя проекта. Можно создать новое дерево в ансамбль. Дерево задаётся не глубиной (что не есть ОК), а отрезаемым от базы данных куском для обучения. Его размер задаётся слайдером "Максимальный размер сэмпла", если не поставлена галка в чекбокс "случайный сэмпл". Если не стоит галка "случайные аттрибуты", то их можно задать руками в окне ниже. После того, как гиперпараметры дерева установлены кнопка "добавить дерево" создаст это дерево и оно отобразится в правой панели.

## Создаём проект

Нажимаем на "Новый проект" или ctrl+N, задаём имя проекта, выбираем базу данных. БД тут понятие несколько условное, поскольку в качестве базы используется просто csv файл в формате

> название-аттрибута-1, ... название-аттрибута-n, лейбл

Аттрибуты могут быть как числовыми, так и строковыми. Впрочем, строковые параметры в этой реализации не кодируются, что, опять же, не есть гуд.

После этого можно создавать и тестировать деревья. См. предыдущий параграф.

# Дополнительно + ссылки по лицензии

В папке лежат два тестовых проекта и, соотв. файла тестовых данных. Один игрушечный, для распознания покемонов, где данные получаются генератором pokemon_generator.py, другой более серьёзный:  
[Gender Recognition by Voice](https://www.kaggle.com/primaryobjects/voicegender) <a style="color:darkgray; font-weight: bold;" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>  
от
[Kory Becker](https://www.kaggle.com/primaryobjects) с Кеггла. Сразу скажу, что прога для того чтобы "поиграться", а не учавствовать в каких-либо соревнованиях на серьёзных щах. Но тем не менее, ансамбль деревьев даёт результат вплоть до 97% правильных ответов на датасете. Понятное дело, что с деревьями решений можно хоть 100 получить, но это просто будет решение датасета, а не доменной задачи. Лучше глубину дерева всё-таки держать под контролем, а ещё лучше использовать какой-нибудь [XGBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn-ensemble-gradientboostingclassifier)
